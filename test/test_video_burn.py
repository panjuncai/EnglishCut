#!/usr/bin/env python3
"""
æµ‹è¯•è§†é¢‘çƒ§åˆ¶åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from video_subtitle_burner import video_burner
from database import db_manager
from logger import LOG

def test_burn_preview():
    """æµ‹è¯•çƒ§åˆ¶é¢„è§ˆåŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•çƒ§åˆ¶é¢„è§ˆåŠŸèƒ½...")
    
    # è·å–ç¬¬ä¸€ä¸ªç³»åˆ—
    series_list = db_manager.get_series()
    if not series_list:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç³»åˆ—")
        return
    
    series_id = series_list[0]['id']
    print(f"ğŸ“¹ ä½¿ç”¨ç³»åˆ—ID: {series_id}")
    
    # è·å–é¢„è§ˆä¿¡æ¯
    preview = video_burner.get_burn_preview(series_id)
    
    print("\nğŸ“Š é¢„è§ˆç»“æœ:")
    print(f"- é‡ç‚¹å•è¯æ•°: {preview['total_keywords']}")
    print(f"- çƒ§åˆ¶æ—¶é•¿: {preview['total_duration']}ç§’")
    print(f"- é¢„ä¼°å¤§å°: {preview['estimated_file_size']}")
    print(f"- è¯é¢‘åˆ†å¸ƒ: {preview['coca_distribution']}")
    
    print("\nğŸ”¤ ç¤ºä¾‹å•è¯:")
    for i, kw in enumerate(preview['sample_keywords'][:3], 1):
        print(f"{i}. {kw['keyword']} {kw['phonetic']} - {kw['explanation']} (COCA: {kw['coca_rank']})")

def test_keyword_selection():
    """æµ‹è¯•å…³é”®è¯é€‰æ‹©é€»è¾‘"""
    print("\nğŸ§  æµ‹è¯•å…³é”®è¯é€‰æ‹©é€»è¾‘...")
    
    # æ¨¡æ‹Ÿå¤šä¸ªå…³é”®è¯
    test_keywords = [
        {'key_word': 'computer', 'coca': 10000, 'phonetic_symbol': '/kÉ™mËˆpjuËtÉ™r/', 'explain_text': 'è®¡ç®—æœº'},
        {'key_word': 'technology', 'coca': 15000, 'phonetic_symbol': '/tekËˆnÉ’lÉ™dÊ’i/', 'explain_text': 'æŠ€æœ¯'},
        {'key_word': 'AI', 'coca': 20000, 'phonetic_symbol': '/ËŒeÉª ËˆaÉª/', 'explain_text': 'äººå·¥æ™ºèƒ½'}
    ]
    
    selected = video_burner._select_most_important_keyword(test_keywords)
    print(f"é€‰æ‹©çš„å…³é”®è¯: {selected['key_word']} (COCA: {selected['coca']})")
    
    # æµ‹è¯•ç›¸åŒCOCAæ’åçš„æƒ…å†µ
    test_keywords_same_coca = [
        {'key_word': 'artificial intelligence', 'coca': 20000, 'phonetic_symbol': '/ËŒÉ‘ËrtÉªËˆfÉªÊƒl ÉªnËˆtelÉªdÊ’É™ns/', 'explain_text': 'äººå·¥æ™ºèƒ½'},
        {'key_word': 'AI', 'coca': 20000, 'phonetic_symbol': '/ËŒeÉª ËˆaÉª/', 'explain_text': 'äººå·¥æ™ºèƒ½'}
    ]
    
    selected_same = video_burner._select_most_important_keyword(test_keywords_same_coca)
    print(f"ç›¸åŒCOCAæ’åæ—¶é€‰æ‹©: {selected_same['key_word']} (é•¿åº¦æ›´çŸ­)")

def test_subtitle_generation():
    """æµ‹è¯•å­—å¹•æ–‡ä»¶ç”Ÿæˆ"""
    print("\nğŸ“ æµ‹è¯•å­—å¹•æ–‡ä»¶ç”Ÿæˆ...")
    
    # æ¨¡æ‹Ÿçƒ§åˆ¶æ•°æ®
    test_burn_data = [
        {
            'begin_time': 0.0,
            'end_time': 2.5,
            'keyword': 'technology',
            'phonetic': '/tekËˆnÉ’lÉ™dÊ’i/',
            'explanation': 'æŠ€æœ¯',
            'coca_rank': 15000
        },
        {
            'begin_time': 5.0,
            'end_time': 7.2,
            'keyword': 'artificial intelligence',
            'phonetic': '/ËŒÉ‘ËrtÉªËˆfÉªÊƒl ÉªnËˆtelÉªdÊ’É™ns/',
            'explanation': 'äººå·¥æ™ºèƒ½',
            'coca_rank': 20000
        }
    ]
    
    # åˆ›å»ºä¸´æ—¶å­—å¹•æ–‡ä»¶
    subtitle_path = "test_keywords.srt"
    video_burner.create_subtitle_file(test_burn_data, subtitle_path)
    
    # è¯»å–å¹¶æ˜¾ç¤ºå†…å®¹
    if os.path.exists(subtitle_path):
        print(f"âœ… å­—å¹•æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {subtitle_path}")
        with open(subtitle_path, 'r', encoding='utf-8') as f:
            content = f.read()
            print("å­—å¹•å†…å®¹:")
            print(content)
        
        # æ¸…ç†
        os.remove(subtitle_path)
        print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
    else:
        print("âŒ å­—å¹•æ–‡ä»¶åˆ›å»ºå¤±è´¥")

def test_database_keywords():
    """æµ‹è¯•æ•°æ®åº“ä¸­çš„å…³é”®è¯æ•°æ®"""
    print("\nğŸ—„ï¸ æµ‹è¯•æ•°æ®åº“å…³é”®è¯æ•°æ®...")
    
    # è·å–ç¬¬ä¸€ä¸ªç³»åˆ—
    series_list = db_manager.get_series()
    if not series_list:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç³»åˆ—")
        return
    
    series_id = series_list[0]['id']
    print(f"ğŸ“¹ æ£€æŸ¥ç³»åˆ—ID: {series_id}")
    
    # è·å–å…³é”®è¯
    keywords = db_manager.get_keywords(series_id=series_id)
    print(f"ğŸ“š æ€»å…³é”®è¯æ•°: {len(keywords)}")
    
    # ç­›é€‰ç¬¦åˆçƒ§åˆ¶æ¡ä»¶çš„å…³é”®è¯
    eligible_keywords = []
    for keyword in keywords:
        coca_rank = keyword.get('coca')
        if coca_rank and coca_rank > 5000:
            eligible_keywords.append(keyword)
    
    print(f"ğŸ¯ ç¬¦åˆçƒ§åˆ¶æ¡ä»¶çš„å…³é”®è¯æ•° (COCA > 5000): {len(eligible_keywords)}")
    
    # æ˜¾ç¤ºå‰5ä¸ªç¬¦åˆæ¡ä»¶çš„å…³é”®è¯
    print("\nå‰5ä¸ªç¬¦åˆæ¡ä»¶çš„å…³é”®è¯:")
    for i, kw in enumerate(eligible_keywords[:5], 1):
        print(f"{i}. {kw['key_word']} (COCA: {kw.get('coca')}) - {kw.get('explain_text', '')}")

if __name__ == "__main__":
    print("ğŸ¬ è§†é¢‘çƒ§åˆ¶åŠŸèƒ½æµ‹è¯•\n")
    
    try:
        test_database_keywords()
        test_keyword_selection()
        test_subtitle_generation()
        test_burn_preview()
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“‹ æ¥ä¸‹æ¥å¯ä»¥å°è¯•:")
        print("1. åœ¨ç•Œé¢ä¸­é€‰æ‹©ä¸€ä¸ªç³»åˆ—ID")
        print("2. ç‚¹å‡»'é¢„è§ˆçƒ§åˆ¶ä¿¡æ¯'æŸ¥çœ‹ç»Ÿè®¡")
        print("3. è®¾ç½®è¾“å‡ºç›®å½•å¹¶ç‚¹å‡»'å¼€å§‹çƒ§åˆ¶'")
        print("4. ç­‰å¾…FFmpegå¤„ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 