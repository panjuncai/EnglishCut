from transformers import pipeline
import gradio as gr
import torch
import tempfile
import os
import subprocess
import json
from datetime import timedelta
from openai import OpenAI # <-- è¿™é‡Œæ˜¯åŸç”Ÿçš„ openai å®¢æˆ·ç«¯
from dotenv import load_dotenv
from openai_translate import translate_text

from logger import LOG

# æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œæ¨¡å‹é…ç½®
def get_optimal_config():
    """æ ¹æ®å¯ç”¨ç¡¬ä»¶é€‰æ‹©æœ€ä¼˜é…ç½®"""
    if torch.cuda.is_available():
        device = "cuda:0"
        model_name = "openai/whisper-large-v3"
        batch_size = 4
        torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
        LOG.info("ğŸš€ ä½¿ç”¨ NVIDIA GPU (CUDA) åŠ é€Ÿ")
    elif torch.backends.mps.is_available():
        device = "mps" 
        model_name = "openai/whisper-base"  # æ›´å¿«çš„æ¨¡å‹é€‚åˆ Mac
        batch_size = 8  # Mac M4 å¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
        LOG.info("ğŸš€ ä½¿ç”¨ Mac GPU (MPS) åŠ é€Ÿ - å·²é’ˆå¯¹ Mac M4 ä¼˜åŒ–")
    else:
        device = "cpu"
        model_name = "openai/whisper-base"  # CPU ç”¨å°æ¨¡å‹
        batch_size = 2  # CPU ç”¨å°æ‰¹æ¬¡
        LOG.warning("âš ï¸  ä½¿ç”¨ CPU å¤„ç†ï¼Œé€Ÿåº¦ä¼šè¾ƒæ…¢")
    
    LOG.info(f"ğŸ“Š é…ç½®: è®¾å¤‡={device}, æ¨¡å‹={model_name}, æ‰¹æ¬¡å¤§å°={batch_size}")
    return device, model_name, batch_size

# è·å–æœ€ä¼˜é…ç½®
device, MODEL_NAME, BATCH_SIZE = get_optimal_config()
CHUNK_LENGTH = 30  # æ¯ä¸ªéŸ³é¢‘ç‰‡æ®µçš„é•¿åº¦ï¼ˆç§’ï¼‰

# åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«ç®¡é“
LOG.info(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME}...")
try:
    # æ ¹æ®è®¾å¤‡ç±»å‹ä¼˜åŒ–æ¨¡å‹å‚æ•°
    if device == "mps":
        # Mac MPS ä¼˜åŒ–é…ç½®
        model_kwargs = {
            "low_cpu_mem_usage": True,
            "use_safetensors": True,
        }
    elif device == "cuda:0":
        # CUDA ä¼˜åŒ–é…ç½®
        model_kwargs = {
            "low_cpu_mem_usage": True,
            "torch_dtype": torch.float16,  # ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
        }
    else:
        # CPU é…ç½®
        model_kwargs = {"low_cpu_mem_usage": True}
    
    pipe = pipeline(
        task="automatic-speech-recognition",
        model=MODEL_NAME,
        chunk_length_s=CHUNK_LENGTH,
        device=device,
        model_kwargs=model_kwargs
    )
    LOG.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è¿è¡Œåœ¨ {device} ä¸Š")
except Exception as e:
    LOG.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    # å›é€€åˆ° CPU å’ŒåŸºç¡€æ¨¡å‹
    LOG.info("ğŸ”„ å°è¯•ä½¿ç”¨ CPU å’ŒåŸºç¡€æ¨¡å‹...")
    device = "cpu"
    MODEL_NAME = "openai/whisper-base"
    BATCH_SIZE = 2
    pipe = pipeline(
        task="automatic-speech-recognition",
        model=MODEL_NAME,
        chunk_length_s=CHUNK_LENGTH,
        device=device,
        model_kwargs={"low_cpu_mem_usage": True}
    )
    LOG.info("âœ… å·²å›é€€åˆ° CPU æ¨¡å¼")

def convert_to_wav(input_path):
    """
    å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸º WAV æ ¼å¼å¹¶è¿”å›æ–°æ–‡ä»¶è·¯å¾„ã€‚

    å‚æ•°:
    - input_path: è¾“å…¥çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„

    è¿”å›:
    - output_path: è½¬æ¢åçš„ WAV æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºä¸´æ—¶ WAV æ–‡ä»¶ï¼Œç”¨äºå­˜å‚¨è½¬æ¢ç»“æœ
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav_file:
        output_path = temp_wav_file.name

    try:
        # ä½¿ç”¨ ffmpeg å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼
        subprocess.run(
            ["ffmpeg", "-y", "-i", input_path, "-ar", "16000", "-ac", "1", output_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        return output_path
    except subprocess.CalledProcessError as e:
        LOG.error(f"éŸ³é¢‘æ–‡ä»¶è½¬æ¢å¤±è´¥: {e}")
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶å¹¶æŠ›å‡ºé”™è¯¯
        if os.path.exists(output_path):
            os.remove(output_path)
        raise gr.Error("éŸ³é¢‘æ–‡ä»¶è½¬æ¢å¤±è´¥ã€‚è¯·ä¸Šä¼ æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶ã€‚")
    except FileNotFoundError:
        LOG.error("æœªæ‰¾åˆ° ffmpeg å¯æ‰§è¡Œæ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²å®‰è£… ffmpegã€‚")
        if os.path.exists(output_path):
            os.remove(output_path)
        raise gr.Error("æœåŠ¡å™¨é…ç½®é”™è¯¯ï¼Œç¼ºå°‘ ffmpegã€‚è¯·è”ç³»ç®¡ç†å‘˜ã€‚")

def asr(audio_file, task="transcribe", return_bilingual=False):
    """
    å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«æˆ–ç¿»è¯‘ã€‚

    å‚æ•°:
    - audio_file: è¾“å…¥çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    - task: ä»»åŠ¡ç±»å‹ï¼ˆ"transcribe" è¡¨ç¤ºè½¬å½•ï¼Œ"translate" è¡¨ç¤ºç¿»è¯‘ï¼‰
    - return_bilingual: æ˜¯å¦è¿”å›åŒè¯­ç»“æœï¼ˆè‹±æ–‡+ä¸­æ–‡ç¿»è¯‘ï¼‰

    è¿”å›:
    - è¯†åˆ«æˆ–ç¿»è¯‘åçš„ç»“æœæ•°æ®
    """
    import time
    start_time = time.time()
    
    # è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
    file_size = os.path.getsize(audio_file) / (1024 * 1024)  # MB
    LOG.info(f"ğŸµ å¼€å§‹å¤„ç†éŸ³é¢‘æ–‡ä»¶: {os.path.basename(audio_file)} ({file_size:.1f}MB)")
    
    # è½¬æ¢éŸ³é¢‘æ–‡ä»¶ä¸º WAV æ ¼å¼
    LOG.info("ğŸ”„ è½¬æ¢éŸ³é¢‘æ ¼å¼...")
    wav_file = convert_to_wav(audio_file)
    conversion_time = time.time() - start_time
    LOG.info(f"âœ… éŸ³é¢‘è½¬æ¢å®Œæˆ ({conversion_time:.1f}ç§’)")

    try:
        # ä½¿ç”¨ç®¡é“è¿›è¡Œè½¬å½•æˆ–ç¿»è¯‘
        LOG.info(f"ğŸ¤– å¼€å§‹è¯­éŸ³è¯†åˆ« (è®¾å¤‡: {device}, æ‰¹æ¬¡: {BATCH_SIZE})...")
        inference_start = time.time()
        
        # é¦–å…ˆè¿›è¡Œè‹±æ–‡è½¬å½•
        result = pipe(
            wav_file,
            batch_size=BATCH_SIZE,
            generate_kwargs={"task": "transcribe", "language": "en"},
            return_timestamps=True
        )
        
        english_text = result["text"]
        chunks = result.get("chunks", [])
        
        # å¦‚æœéœ€è¦åŒè¯­ï¼Œç¿»è¯‘è‹±æ–‡ä¸ºä¸­æ–‡
        chinese_chunks = []
        chinese_text = ""
        
        if return_bilingual:
            LOG.info("ğŸŒ å¼€å§‹ä½¿ç”¨GPT-4o-miniç”Ÿæˆä¸­æ–‡ç¿»è¯‘...")
            
            # ç¿»è¯‘æ•´ä½“æ–‡æœ¬
            chinese_text = translate_text(english_text)
            
            # ç¿»è¯‘æ¯ä¸ªæ—¶é—´æˆ³ç‰‡æ®µ
            for chunk in chunks:
                english_chunk_text = chunk.get("text", "").strip()
                if english_chunk_text:
                    chinese_chunk_text = translate_text(english_chunk_text)
                    chinese_chunks.append({
                        "text": chinese_chunk_text,
                        "timestamp": chunk.get("timestamp", [None, None])
                    })
        
        inference_time = time.time() - inference_start
        total_time = time.time() - start_time
        
        # è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆä¼°ç®—ï¼‰
        audio_duration = len(english_text.split()) * 0.6  # ç²—ç•¥ä¼°ç®—ï¼šæ¯ä¸ªå•è¯0.6ç§’
        speed_ratio = audio_duration / total_time if total_time > 0 else 0
        
        LOG.info(f"âœ… è¯†åˆ«å®Œæˆ! æ€»æ—¶é•¿: {total_time:.1f}ç§’, æ¨ç†: {inference_time:.1f}ç§’")
        LOG.info(f"âš¡ å¤„ç†é€Ÿåº¦: {speed_ratio:.1f}x å®æ—¶é€Ÿåº¦")
        LOG.info(f"ğŸ“ è‹±æ–‡ç»“æœ ({len(english_text)} å­—ç¬¦): {english_text[:100]}...")
        if return_bilingual:
            LOG.info(f"ğŸŒ ä¸­æ–‡ç¿»è¯‘ ({len(chinese_text)} å­—ç¬¦): {chinese_text[:100]}...")
        LOG.info(f"ğŸ• æ—¶é—´æˆ³ç‰‡æ®µæ•°: {len(chunks)}")

        return {
            "english_text": english_text,
            "chinese_text": chinese_text if return_bilingual else "",
            "english_chunks": chunks,
            "chinese_chunks": chinese_chunks if return_bilingual else [],
            "text": english_text,  # ä¿æŒå…¼å®¹æ€§
            "chunks": chunks,     # ä¿æŒå…¼å®¹æ€§
            "processing_time": total_time,
            "is_bilingual": return_bilingual
        }
    except Exception as e:
        LOG.error(f"âŒ å¤„ç†éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise gr.Error(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
    finally:
        # åˆ é™¤ä¸´æ—¶è½¬æ¢åçš„ WAV æ–‡ä»¶
        if os.path.exists(wav_file):
            os.remove(wav_file)

def format_time_lrc(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸ºLRCæ ¼å¼æ—¶é—´æˆ³ [mm:ss.xx]"""
    if seconds is None:
        return "[00:00.00]"
    
    minutes = int(seconds // 60)
    secs = seconds % 60
    return f"[{minutes:02d}:{secs:05.2f}]"

def align_bilingual_chunks(english_chunks, chinese_chunks):
    """å¯¹é½è‹±æ–‡å’Œä¸­æ–‡å­—å¹•å—"""
    aligned_chunks = []
    
    # ç›´æ¥ä½¿ç”¨è‹±æ–‡æ—¶é—´æˆ³å’Œå¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘
    for i, eng_chunk in enumerate(english_chunks):
        english_text = eng_chunk.get("text", "").strip()
        timestamp = eng_chunk.get("timestamp", [None, None])
        
        # è·å–å¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘
        if i < len(chinese_chunks):
            chinese_text = chinese_chunks[i].get("text", "").strip()
        else:
            chinese_text = ""
        
        aligned_chunks.append({
            "timestamp": timestamp,
            "english": english_text,
            "chinese": chinese_text
        })
    
    return aligned_chunks

def generate_lrc_content(result_data, audio_filename="audio"):
    """ç”ŸæˆLRCæ ¼å¼å­—å¹•å†…å®¹ï¼Œæ”¯æŒåŒè¯­"""
    if not isinstance(result_data, dict):
        return ""
    
    is_bilingual = result_data.get("is_bilingual", False)
    processing_time = result_data.get("processing_time", 0)
    
    lrc_lines = []
    
    # LRCæ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
    lrc_lines.append("[ti:Audio Transcription]")
    lrc_lines.append(f"[ar:Generated by EnglishCut]")
    lrc_lines.append(f"[al:{audio_filename}]")
    lrc_lines.append(f"[by:OpenAI Whisper]")
    lrc_lines.append(f"[offset:0]")
    lrc_lines.append("")
    
    if is_bilingual:
        # åŒè¯­æ¨¡å¼
        english_chunks = result_data.get("english_chunks", [])
        chinese_chunks = result_data.get("chinese_chunks", [])
        
        if english_chunks:
            # å¯¹é½è‹±ä¸­å­—å¹•
            aligned_chunks = align_bilingual_chunks(english_chunks, chinese_chunks)
            
            for chunk in aligned_chunks:
                timestamp = chunk.get("timestamp", [None, None])
                english_text = chunk.get("english", "")
                chinese_text = chunk.get("chinese", "")
                
                if english_text and timestamp[0] is not None:
                    time_tag = format_time_lrc(timestamp[0])
                    if chinese_text:
                        # åŒè¯­æ ¼å¼ï¼šè‹±æ–‡ // ä¸­æ–‡
                        lrc_lines.append(f"{time_tag}{english_text} // {chinese_text}")
                    else:
                        # åªæœ‰è‹±æ–‡
                        lrc_lines.append(f"{time_tag}{english_text}")
        else:
            # æ²¡æœ‰æ—¶é—´æˆ³ï¼Œä½¿ç”¨æ•´æ®µæ–‡æœ¬
            english_text = result_data.get("english_text", "")
            chinese_text = result_data.get("chinese_text", "")
            if english_text:
                bilingual_text = f"{english_text} // {chinese_text}" if chinese_text else english_text
                lrc_lines.append("[00:00.00]" + bilingual_text)
    else:
        # å•è¯­æ¨¡å¼ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        text = result_data.get("text", "")
        chunks = result_data.get("chunks", [])
        
        if chunks:
            for chunk in chunks:
                timestamp = chunk.get("timestamp", [None, None])
                chunk_text = chunk.get("text", "").strip()
                
                if chunk_text and timestamp[0] is not None:
                    time_tag = format_time_lrc(timestamp[0])
                    lrc_lines.append(f"{time_tag}{chunk_text}")
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œæ·»åŠ æ•´ä¸ªæ–‡æœ¬
            lrc_lines.append("[00:00.00]" + text)
    
    # æ·»åŠ ç»“æŸæ ‡è®°
    lrc_lines.append("")
    lrc_lines.append(f"[99:59.99]Generated in {processing_time:.1f} seconds")
    
    return "\n".join(lrc_lines)

def save_lrc_file(result_data, audio_filepath):
    """ä¿å­˜LRCå­—å¹•æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    try:
        # è·å–éŸ³é¢‘æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        audio_name = os.path.splitext(os.path.basename(audio_filepath))[0]
        
        # ç”ŸæˆLRCå†…å®¹
        lrc_content = generate_lrc_content(result_data, audio_name)
        
        # åˆ›å»ºLRCæ–‡ä»¶è·¯å¾„
        lrc_filename = f"{audio_name}_subtitle.lrc"
        lrc_filepath = os.path.join(tempfile.gettempdir(), lrc_filename)
        
        # å†™å…¥LRCæ–‡ä»¶
        with open(lrc_filepath, 'w', encoding='utf-8') as f:
            f.write(lrc_content)
        
        LOG.info(f"ğŸ“ LRCå­—å¹•æ–‡ä»¶å·²ç”Ÿæˆ: {lrc_filepath}")
        return lrc_filepath
        
    except Exception as e:
        LOG.error(f"âŒ ç”ŸæˆLRCæ–‡ä»¶å¤±è´¥: {e}")
        return None

def format_time_srt(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ ¼å¼æ—¶é—´æˆ³ [hh:mm:ss,mmm]"""
    if seconds is None:
        return "00:00:00,000"
    
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    milliseconds = int((secs % 1) * 1000)
    secs = int(secs)
    
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"

def generate_srt_content(result_data, audio_filename="audio"):
    """ç”ŸæˆSRTæ ¼å¼å­—å¹•å†…å®¹ï¼Œæ”¯æŒåŒè¯­"""
    if not isinstance(result_data, dict):
        return ""
    
    is_bilingual = result_data.get("is_bilingual", False)
    
    srt_lines = []
    subtitle_index = 1
    
    if is_bilingual:
        # åŒè¯­æ¨¡å¼
        english_chunks = result_data.get("english_chunks", [])
        chinese_chunks = result_data.get("chinese_chunks", [])
        
        if english_chunks:
            # å¯¹é½è‹±ä¸­å­—å¹•
            aligned_chunks = align_bilingual_chunks(english_chunks, chinese_chunks)
            
            for chunk in aligned_chunks:
                timestamp = chunk.get("timestamp", [None, None])
                english_text = chunk.get("english", "")
                chinese_text = chunk.get("chinese", "")
                
                if english_text and timestamp[0] is not None and timestamp[1] is not None:
                    start_time = format_time_srt(timestamp[0])
                    end_time = format_time_srt(timestamp[1])
                    
                    # SRT æ ¼å¼ï¼šåºå·ã€æ—¶é—´æˆ³ã€å­—å¹•å†…å®¹ã€ç©ºè¡Œ
                    srt_lines.append(str(subtitle_index))
                    srt_lines.append(f"{start_time} --> {end_time}")
                    
                    if chinese_text:
                        # åŒè¯­æ ¼å¼ï¼šè‹±æ–‡æ¢è¡Œä¸­æ–‡
                        srt_lines.append(english_text)
                        srt_lines.append(chinese_text)
                    else:
                        # åªæœ‰è‹±æ–‡
                        srt_lines.append(english_text)
                    
                    srt_lines.append("")  # ç©ºè¡Œåˆ†éš”
                    subtitle_index += 1
        else:
            # æ²¡æœ‰æ—¶é—´æˆ³ï¼Œä½¿ç”¨æ•´æ®µæ–‡æœ¬
            english_text = result_data.get("english_text", "")
            chinese_text = result_data.get("chinese_text", "")
            if english_text:
                srt_lines.append("1")
                srt_lines.append("00:00:00,000 --> 00:00:10,000")
                srt_lines.append(english_text)
                if chinese_text:
                    srt_lines.append(chinese_text)
                srt_lines.append("")
    else:
        # å•è¯­æ¨¡å¼ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        text = result_data.get("text", "")
        chunks = result_data.get("chunks", [])
        
        if chunks:
            for chunk in chunks:
                timestamp = chunk.get("timestamp", [None, None])
                chunk_text = chunk.get("text", "").strip()
                
                if chunk_text and timestamp[0] is not None and timestamp[1] is not None:
                    start_time = format_time_srt(timestamp[0])
                    end_time = format_time_srt(timestamp[1])
                    
                    srt_lines.append(str(subtitle_index))
                    srt_lines.append(f"{start_time} --> {end_time}")
                    srt_lines.append(chunk_text)
                    srt_lines.append("")  # ç©ºè¡Œåˆ†éš”
                    subtitle_index += 1
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œæ·»åŠ æ•´ä¸ªæ–‡æœ¬
            srt_lines.append("1")
            srt_lines.append("00:00:00,000 --> 00:00:10,000")
            srt_lines.append(text)
            srt_lines.append("")
    
    return "\n".join(srt_lines)

def save_srt_file(result_data, audio_filepath):
    """ä¿å­˜SRTå­—å¹•æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    try:
        # è·å–éŸ³é¢‘æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        audio_name = os.path.splitext(os.path.basename(audio_filepath))[0]
        
        # ç”ŸæˆSRTå†…å®¹
        srt_content = generate_srt_content(result_data, audio_name)
        
        # åˆ›å»ºSRTæ–‡ä»¶è·¯å¾„
        srt_filename = f"{audio_name}_subtitle.srt"
        srt_filepath = os.path.join(tempfile.gettempdir(), srt_filename)
        
        # å†™å…¥SRTæ–‡ä»¶
        with open(srt_filepath, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        LOG.info(f"ğŸ“ SRTå­—å¹•æ–‡ä»¶å·²ç”Ÿæˆ: {srt_filepath}")
        return srt_filepath
        
    except Exception as e:
        LOG.error(f"âŒ ç”ŸæˆSRTæ–‡ä»¶å¤±è´¥: {e}")
        return None

def transcribe(inputs, task):
    """
    å°†éŸ³é¢‘æ–‡ä»¶è½¬å½•æˆ–ç¿»è¯‘ä¸ºæ–‡æœ¬ã€‚

    å‚æ•°:
    - inputs: ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    - task: ä»»åŠ¡ç±»å‹ï¼ˆ"transcribe" è¡¨ç¤ºè½¬å½•ï¼Œ"translate" è¡¨ç¤ºç¿»è¯‘ï¼‰

    è¿”å›:
    - è¯†åˆ«çš„æ–‡æœ¬å†…å®¹
    """
    LOG.info(f"[ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶]: {inputs}")

    # æ£€æŸ¥æ˜¯å¦æä¾›äº†éŸ³é¢‘æ–‡ä»¶
    if not inputs or not os.path.exists(inputs):
        raise gr.Error("æœªæäº¤éŸ³é¢‘æ–‡ä»¶ï¼è¯·åœ¨æäº¤è¯·æ±‚å‰ä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘æ–‡ä»¶ã€‚")

    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼
    file_ext = os.path.splitext(inputs)[1].lower()
    if file_ext not in ['.wav', '.flac', '.mp3']:
        LOG.error(f"æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼š{inputs}")
        raise gr.Error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼è¯·ä¸Šä¼  WAVã€FLAC æˆ– MP3 æ–‡ä»¶ã€‚")

    # è°ƒç”¨è¯­éŸ³è¯†åˆ«æˆ–ç¿»è¯‘å‡½æ•°
    return asr(inputs, task)

# å®šä¹‰éº¦å…‹é£è¾“å…¥çš„æ¥å£å®ä¾‹ï¼Œå¯ä¾›å¤–éƒ¨æ¨¡å—è°ƒç”¨
mf_transcribe = gr.Interface(
    fn=transcribe,  # æ‰§è¡Œè½¬å½•çš„å‡½æ•°
    inputs=[
        gr.Audio(source="microphone", type="filepath", label="éº¦å…‹é£è¾“å…¥"),  # ä½¿ç”¨éº¦å…‹é£å½•åˆ¶çš„éŸ³é¢‘è¾“å…¥
        gr.Radio(choices=["transcribe", "translate"], label="ä»»åŠ¡ç±»å‹", value="transcribe"),  # ä»»åŠ¡é€‰æ‹©ï¼ˆè½¬å½•æˆ–ç¿»è¯‘ï¼‰
    ],
    outputs=gr.Textbox(label="è¯†åˆ«ç»“æœ"),  # è¾“å‡ºä¸ºæ–‡æœ¬
    title="Whisper Large V3: è¯­éŸ³è¯†åˆ«",  # æ¥å£æ ‡é¢˜
    description="ä½¿ç”¨éº¦å…‹é£å½•åˆ¶éŸ³é¢‘å¹¶è¿›è¡Œè¯­éŸ³è¯†åˆ«æˆ–ç¿»è¯‘ã€‚",  # æ¥å£æè¿°
)

# å®šä¹‰æ–‡ä»¶ä¸Šä¼ çš„æ¥å£å®ä¾‹ï¼Œç”¨äºå¤„ç†ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
file_transcribe = gr.Interface(
    fn=transcribe,  # æ‰§è¡Œè½¬å½•çš„å‡½æ•°
    inputs=[
        gr.Audio(source="upload", type="filepath", label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"),  # ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶è¾“å…¥
        gr.Radio(choices=["transcribe", "translate"], label="ä»»åŠ¡ç±»å‹", value="transcribe"),  # ä»»åŠ¡é€‰æ‹©ï¼ˆè½¬å½•æˆ–ç¿»è¯‘ï¼‰
    ],
    outputs=gr.Textbox(label="è¯†åˆ«ç»“æœ"),  # è¾“å‡ºä¸ºæ–‡æœ¬
    title="Whisper Large V3: è½¬å½•éŸ³é¢‘æ–‡ä»¶",  # æ¥å£æ ‡é¢˜
    description="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆWAVã€FLAC æˆ– MP3ï¼‰å¹¶è¿›è¡Œè¯­éŸ³è¯†åˆ«æˆ–ç¿»è¯‘ã€‚",  # æ¥å£æè¿°
)

# ä»…å½“æ­¤è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œæ—¶ï¼Œæ‰§è¡Œ Gradio åº”ç”¨çš„å¯åŠ¨ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ª Gradio Blocks å®ä¾‹ï¼Œç”¨äºåŒ…å«å¤šä¸ªæ¥å£
    with gr.Blocks() as demo:
        # ä½¿ç”¨ TabbedInterface å°† mf_transcribe å’Œ file_transcribe æ¥å£åˆ†åˆ«æ”¾ç½®åœ¨ "éº¦å…‹é£" å’Œ "éŸ³é¢‘æ–‡ä»¶" é€‰é¡¹å¡ä¸­
        gr.TabbedInterface(
            [mf_transcribe, file_transcribe],
            ["éº¦å…‹é£", "éŸ³é¢‘æ–‡ä»¶"]
        )

    # å¯åŠ¨Gradioåº”ç”¨ï¼Œå…è®¸é˜Ÿåˆ—åŠŸèƒ½ï¼Œå¹¶é€šè¿‡ HTTPS è®¿é—®
    demo.queue().launch(
        share=False,
        server_name="127.0.0.1",
        # auth=("django", "1234") # âš ï¸æ³¨æ„ï¼šè®°ä½ä¿®æ”¹å¯†ç 
    )
